{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_Q35Vrxrk1O"
      },
      "source": [
        "# SAM 3 Segmentation for 3DGS (Official API)\n",
        "\n",
        "**Based on**: Official SAM 3 examples from Facebook Research\n",
        "\n",
        "**Features**:\n",
        "- ‚úÖ Text prompts (\"green frog\", \"pink flower\")\n",
        "- ‚úÖ Batched inference for speed\n",
        "- ‚úÖ Automatic confidence filtering\n",
        "- ‚úÖ ZIP or Video input\n",
        "\n",
        "**Source**: `sam3_image_predictor_example.ipynb` + `sam3_image_batched_inference.ipynb`\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxPbaE4nrk1P"
      },
      "source": [
        "# Part 1: Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi0b4OJXrk1Q"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Drive mounted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwrroGgPrk1R"
      },
      "outputs": [],
      "source": [
        "print(\"üîß Installing SAM 3 (fixed method)...\\n\")\n",
        "\n",
        "# Cleanup\n",
        "print(\"[1/6] Cleanup...\")\n",
        "!pip uninstall -y -q jax jaxlib pytensor shap music21 2>/dev/null || true\n",
        "!pip uninstall -y -q opencv-python opencv-contrib-python opencv-python-headless 2>/dev/null || true\n",
        "!pip uninstall -y -q numpy 2>/dev/null || true\n",
        "\n",
        "print(\"[2/6] NumPy 1.26...\")\n",
        "!pip install -q numpy==1.26.0\n",
        "\n",
        "print(\"[3/6] Core dependencies...\")\n",
        "!pip install -q timm>=1.0.17 tqdm ftfy==6.1.1 regex iopath>=0.1.10 typing_extensions huggingface_hub\n",
        "!pip install -q opencv-python==4.9.0.80 matplotlib pillow scikit-learn jupyter ipywidgets\n",
        "\n",
        "print(\"[4/6] Clone SAM 3...\")\n",
        "import os\n",
        "if not os.path.exists('/content/sam3'):\n",
        "    !git clone -q https://github.com/facebookresearch/sam3.git\n",
        "else:\n",
        "    print(\"   Already cloned\")\n",
        "\n",
        "print(\"[5/6] Install SAM 3...\")\n",
        "%cd /content/sam3\n",
        "!pip install -q -e \".[notebooks]\"\n",
        "%cd /content\n",
        "\n",
        "print(\"[6/6] Python path...\")\n",
        "import sys\n",
        "if '/content/sam3' not in sys.path:\n",
        "    sys.path.insert(0, '/content/sam3')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ INSTALLATION COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n‚ö†Ô∏è  RESTART RUNTIME NOW!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHtP6djYrk1S"
      },
      "source": [
        "---\n",
        "# ‚ö†Ô∏è RESTART RUNTIME!\n",
        "Then continue to Part 2 ‚¨áÔ∏è\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ky8_UITrk1T"
      },
      "source": [
        "# Part 2: Setup (After Restart)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMIBcl3Mrk1T"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Drive mounted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43t-b5yqrk1U"
      },
      "outputs": [],
      "source": [
        "print(\"üîç Verifying installation...\\n\")\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# CRITICAL: Add BOTH paths\n",
        "sam3_repo = '/content/sam3'\n",
        "if sam3_repo not in sys.path:\n",
        "    sys.path.insert(0, sam3_repo)\n",
        "\n",
        "# Force reimport if already imported incorrectly\n",
        "if 'sam3' in sys.modules:\n",
        "    del sys.modules['sam3']\n",
        "\n",
        "# Now import\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "print(\"Importing SAM 3 modules...\")\n",
        "\n",
        "# Import from the correct location\n",
        "from sam3.model_builder import build_sam3_image_model  # ‚úÖ Correct import!\n",
        "from sam3.model.sam3_image_processor import Sam3Processor\n",
        "from sam3.visualization_utils import plot_results\n",
        "\n",
        "# Set sam3_root\n",
        "sam3_root = '/content/sam3'\n",
        "\n",
        "print(\"‚úÖ All imports successful!\\n\")\n",
        "print(\"üìä Environment:\")\n",
        "print(f\"   Python: {sys.version.split()[0]}\")\n",
        "print(f\"   PyTorch: {torch.__version__}\")\n",
        "print(f\"   NumPy: {np.__version__}\")\n",
        "print(f\"   OpenCV: {cv2.__version__}\")\n",
        "print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"   SAM 3 root: {sam3_root}\")\n",
        "\n",
        "if np.__version__.startswith('1.26'):\n",
        "    print(\"\\nüéâ Ready for SAM 3!\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è NumPy: {np.__version__} (expected 1.26.x)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "908ij99trk1U"
      },
      "outputs": [],
      "source": [
        "# PyTorch optimization (from official example)\n",
        "import torch\n",
        "\n",
        "# Turn on tfloat32 for Ampere GPUs\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# Use bfloat16 (if your GPU supports it, T4 does)\n",
        "torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "# Inference mode\n",
        "torch.inference_mode().__enter__()\n",
        "\n",
        "print(\"‚úÖ PyTorch optimizations enabled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf8Btlu0rk1V"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "print(\"üîê Hugging Face Login\")\n",
        "print(\"üí° Token: https://huggingface.co/settings/tokens\\n\")\n",
        "\n",
        "login()\n",
        "\n",
        "print(\"\\n‚úÖ Logged in!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTOju2m3rk1Z"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKC5I0uwlNt_",
        "outputId": "dae6d0f9-ad17-413c-8e7d-4241cd7f8d33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Loading SAM 3 model...\n",
            "\n",
            "‚úÖ Model loaded on cuda\n",
            "   GPU Memory: 7.08 GB\n",
            "\n",
            "üé¨ Processing video: IMG_7593.MOV\n",
            "\n",
            "   Output: /content/ydino_0.4\n",
            "   Prompts: ['yellow toy']\n",
            "   Skip: 4s, Sample: 1/6\n",
            "\n",
            "üìä Video info:\n",
            "   FPS: 30.0\n",
            "   Total frames: 1007\n",
            "   Skip frames: 119\n",
            "   Expected output: ~148 frames\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing:  11%|‚ñà‚ñè        | 17/148 [01:50<14:11,  6.50s/it]\n",
            "\n",
            "Processing:   1%|          | 1/148 [00:09<23:53,  9.75s/it]\u001b[A\n",
            "Processing:   1%|‚ñè         | 2/148 [00:12<13:18,  5.47s/it]\u001b[A\n",
            "Processing:   2%|‚ñè         | 3/148 [00:14<10:05,  4.17s/it]\u001b[A\n",
            "Processing:   3%|‚ñé         | 4/148 [00:17<08:36,  3.58s/it]\u001b[A\n",
            "Processing:   3%|‚ñé         | 5/148 [00:20<07:38,  3.21s/it]\u001b[A\n",
            "Processing:   4%|‚ñç         | 6/148 [00:22<07:05,  3.00s/it]\u001b[A\n",
            "Processing:   5%|‚ñç         | 7/148 [00:25<06:44,  2.87s/it]\u001b[A\n",
            "Processing:   5%|‚ñå         | 8/148 [00:28<06:35,  2.83s/it]\u001b[A\n",
            "Processing:   6%|‚ñå         | 9/148 [00:30<06:31,  2.82s/it]\u001b[A\n",
            "Processing:   7%|‚ñã         | 10/148 [00:33<06:20,  2.76s/it]\u001b[A\n",
            "Processing:   7%|‚ñã         | 11/148 [00:36<06:14,  2.73s/it]\u001b[A\n",
            "Processing:   8%|‚ñä         | 12/148 [00:38<06:07,  2.70s/it]\u001b[A\n",
            "Processing:   9%|‚ñâ         | 13/148 [00:41<06:06,  2.71s/it]\u001b[A\n",
            "Processing:   9%|‚ñâ         | 14/148 [00:44<06:07,  2.74s/it]\u001b[A\n",
            "Processing:  10%|‚ñà         | 15/148 [00:46<05:59,  2.70s/it]\u001b[A\n",
            "Processing:  11%|‚ñà         | 16/148 [00:49<05:53,  2.68s/it]\u001b[A\n",
            "Processing:  11%|‚ñà‚ñè        | 17/148 [00:52<05:46,  2.65s/it]\u001b[A\n",
            "Processing:  12%|‚ñà‚ñè        | 18/148 [00:54<05:44,  2.65s/it]\u001b[A\n",
            "Processing:  13%|‚ñà‚ñé        | 19/148 [00:57<05:43,  2.66s/it]\u001b[A\n",
            "Processing:  14%|‚ñà‚ñé        | 20/148 [00:59<05:35,  2.62s/it]\u001b[A\n",
            "Processing:  14%|‚ñà‚ñç        | 21/148 [01:02<05:29,  2.59s/it]\u001b[A\n",
            "Processing:  15%|‚ñà‚ñç        | 22/148 [01:05<05:23,  2.57s/it]\u001b[A\n",
            "Processing:  16%|‚ñà‚ñå        | 23/148 [01:07<05:23,  2.59s/it]\u001b[A\n",
            "Processing:  16%|‚ñà‚ñå        | 24/148 [01:10<05:24,  2.61s/it]\u001b[A\n",
            "Processing:  17%|‚ñà‚ñã        | 25/148 [01:12<05:18,  2.59s/it]\u001b[A\n",
            "Processing:  18%|‚ñà‚ñä        | 26/148 [01:15<05:13,  2.57s/it]\u001b[A\n",
            "Processing:  18%|‚ñà‚ñä        | 27/148 [01:17<05:09,  2.56s/it]\u001b[A\n",
            "Processing:  19%|‚ñà‚ñâ        | 28/148 [01:20<05:09,  2.58s/it]\u001b[A\n",
            "Processing:  20%|‚ñà‚ñâ        | 29/148 [01:23<05:11,  2.62s/it]\u001b[A\n",
            "Processing:  20%|‚ñà‚ñà        | 30/148 [01:25<05:05,  2.59s/it]\u001b[A\n",
            "Processing:  21%|‚ñà‚ñà        | 31/148 [01:28<05:01,  2.57s/it]\u001b[A\n",
            "Processing:  22%|‚ñà‚ñà‚ñè       | 32/148 [01:30<04:57,  2.56s/it]\u001b[A\n",
            "Processing:  22%|‚ñà‚ñà‚ñè       | 33/148 [01:33<04:56,  2.58s/it]\u001b[A\n",
            "Processing:  23%|‚ñà‚ñà‚ñé       | 34/148 [01:36<04:58,  2.62s/it]\u001b[A\n",
            "Processing:  24%|‚ñà‚ñà‚ñé       | 35/148 [01:38<04:53,  2.60s/it]\u001b[A\n",
            "Processing:  24%|‚ñà‚ñà‚ñç       | 36/148 [01:41<04:49,  2.58s/it]\u001b[A\n",
            "Processing:  25%|‚ñà‚ñà‚ñå       | 37/148 [01:43<04:46,  2.58s/it]\u001b[A\n",
            "Processing:  26%|‚ñà‚ñà‚ñå       | 38/148 [01:46<04:46,  2.60s/it]\u001b[A\n",
            "Processing:  26%|‚ñà‚ñà‚ñã       | 39/148 [01:49<04:50,  2.66s/it]\u001b[A\n",
            "Processing:  27%|‚ñà‚ñà‚ñã       | 40/148 [01:51<04:43,  2.63s/it]\u001b[A\n",
            "Processing:  28%|‚ñà‚ñà‚ñä       | 41/148 [01:54<04:38,  2.61s/it]\u001b[A\n",
            "Processing:  28%|‚ñà‚ñà‚ñä       | 42/148 [01:56<04:34,  2.59s/it]\u001b[A\n",
            "Processing:  29%|‚ñà‚ñà‚ñâ       | 43/148 [01:59<04:34,  2.61s/it]\u001b[A\n",
            "Processing:  30%|‚ñà‚ñà‚ñâ       | 44/148 [02:02<04:37,  2.67s/it]\u001b[A\n",
            "Processing:  30%|‚ñà‚ñà‚ñà       | 45/148 [02:05<04:35,  2.67s/it]\u001b[A\n",
            "Processing:  31%|‚ñà‚ñà‚ñà       | 46/148 [02:07<04:29,  2.64s/it]\u001b[A\n",
            "Processing:  32%|‚ñà‚ñà‚ñà‚ñè      | 47/148 [02:10<04:24,  2.62s/it]\u001b[A\n",
            "Processing:  32%|‚ñà‚ñà‚ñà‚ñè      | 48/148 [02:12<04:23,  2.63s/it]\u001b[A\n",
            "Processing:  33%|‚ñà‚ñà‚ñà‚ñé      | 49/148 [02:15<04:28,  2.71s/it]\u001b[A\n",
            "Processing:  34%|‚ñà‚ñà‚ñà‚ñç      | 50/148 [02:18<04:21,  2.67s/it]\u001b[A\n",
            "Processing:  34%|‚ñà‚ñà‚ñà‚ñç      | 51/148 [02:20<04:15,  2.63s/it]\u001b[A\n",
            "Processing:  35%|‚ñà‚ñà‚ñà‚ñå      | 52/148 [02:23<04:09,  2.60s/it]\u001b[A\n",
            "Processing:  36%|‚ñà‚ñà‚ñà‚ñå      | 53/148 [02:26<04:06,  2.59s/it]\u001b[A\n",
            "Processing:  36%|‚ñà‚ñà‚ñà‚ñã      | 54/148 [02:28<04:10,  2.66s/it]\u001b[A\n",
            "Processing:  37%|‚ñà‚ñà‚ñà‚ñã      | 55/148 [02:31<04:07,  2.66s/it]\u001b[A\n",
            "Processing:  38%|‚ñà‚ñà‚ñà‚ñä      | 56/148 [02:34<04:01,  2.62s/it]\u001b[A\n",
            "Processing:  39%|‚ñà‚ñà‚ñà‚ñä      | 57/148 [02:36<03:56,  2.60s/it]\u001b[A\n",
            "Processing:  39%|‚ñà‚ñà‚ñà‚ñâ      | 58/148 [02:39<03:52,  2.58s/it]\u001b[A\n",
            "Processing:  40%|‚ñà‚ñà‚ñà‚ñâ      | 59/148 [02:41<03:53,  2.62s/it]\u001b[A\n",
            "Processing:  41%|‚ñà‚ñà‚ñà‚ñà      | 60/148 [02:44<03:53,  2.65s/it]\u001b[A\n",
            "Processing:  41%|‚ñà‚ñà‚ñà‚ñà      | 61/148 [02:47<03:48,  2.62s/it]\u001b[A\n",
            "Processing:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 62/148 [02:49<03:44,  2.61s/it]\u001b[A\n",
            "Processing:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 63/148 [02:52<03:42,  2.61s/it]\u001b[A\n",
            "Processing:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 64/148 [02:54<03:39,  2.62s/it]\u001b[A\n",
            "Processing:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 65/148 [02:57<03:40,  2.66s/it]\u001b[A\n",
            "Processing:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 66/148 [03:00<03:35,  2.63s/it]\u001b[A\n",
            "Processing:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 67/148 [03:02<03:31,  2.61s/it]\u001b[A\n",
            "Processing:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 68/148 [03:05<03:27,  2.60s/it]\u001b[A\n",
            "Processing:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 69/148 [03:08<03:26,  2.61s/it]\u001b[A\n",
            "Processing:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 70/148 [03:10<03:26,  2.65s/it]\u001b[A\n",
            "Processing:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 71/148 [03:13<03:20,  2.61s/it]\u001b[A\n",
            "Processing:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 72/148 [03:15<03:17,  2.60s/it]\u001b[A\n",
            "Processing:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 73/148 [03:18<03:13,  2.58s/it]\u001b[A\n",
            "Processing:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 74/148 [03:21<03:12,  2.60s/it]\u001b[A\n",
            "Processing:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 75/148 [03:23<03:13,  2.65s/it]\u001b[A\n",
            "Processing:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 76/148 [03:26<03:08,  2.62s/it]\u001b[A\n",
            "Processing:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 77/148 [03:28<03:04,  2.59s/it]\u001b[A\n",
            "Processing:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 78/148 [03:31<03:00,  2.58s/it]\u001b[A\n",
            "Processing:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 79/148 [03:34<03:00,  2.61s/it]\u001b[A\n",
            "Processing:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 80/148 [03:36<02:59,  2.64s/it]\u001b[A\n",
            "Processing:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 81/148 [03:39<02:54,  2.61s/it]\u001b[A\n",
            "Processing:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 82/148 [03:41<02:50,  2.59s/it]\u001b[A\n",
            "Processing:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 83/148 [03:44<02:47,  2.58s/it]\u001b[A\n",
            "Processing:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 84/148 [03:47<02:46,  2.61s/it]\u001b[A\n",
            "Processing:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 85/148 [03:49<02:46,  2.65s/it]\u001b[A\n",
            "Processing:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 86/148 [03:52<02:43,  2.63s/it]\u001b[A\n",
            "Processing:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 87/148 [03:55<02:39,  2.62s/it]\u001b[A\n",
            "Processing:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 88/148 [03:57<02:36,  2.60s/it]\u001b[A\n",
            "Processing:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 89/148 [04:00<02:35,  2.63s/it]\u001b[A\n",
            "Processing:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 90/148 [04:03<02:36,  2.69s/it]\u001b[A\n",
            "Processing:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 91/148 [04:05<02:31,  2.65s/it]\u001b[A\n",
            "Processing:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 92/148 [04:08<02:26,  2.62s/it]\u001b[A\n",
            "Processing:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 93/148 [04:10<02:22,  2.60s/it]\u001b[A\n",
            "Processing:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 94/148 [04:13<02:20,  2.61s/it]\u001b[A\n",
            "Processing:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 95/148 [04:16<02:21,  2.67s/it]\u001b[A\n",
            "Processing:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 96/148 [04:18<02:18,  2.67s/it]\u001b[A\n",
            "Processing:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 97/148 [04:21<02:14,  2.64s/it]\u001b[A\n",
            "Processing:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 98/148 [04:24<02:10,  2.61s/it]\u001b[A\n",
            "Processing:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 99/148 [04:26<02:06,  2.59s/it]\u001b[A\n",
            "Processing:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 100/148 [04:29<02:07,  2.65s/it]\u001b[A\n",
            "Processing:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 101/148 [04:32<02:05,  2.67s/it]\u001b[A\n",
            "Processing:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 102/148 [04:34<02:01,  2.64s/it]\u001b[A\n",
            "Processing:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 103/148 [04:37<01:57,  2.62s/it]\u001b[A\n",
            "Processing:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 104/148 [04:39<01:54,  2.60s/it]\u001b[A\n",
            "Processing:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 105/148 [04:42<01:52,  2.61s/it]\u001b[A\n",
            "Processing:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 106/148 [04:45<01:50,  2.64s/it]\u001b[A\n",
            "Processing:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 107/148 [04:47<01:46,  2.60s/it]\u001b[A\n",
            "Processing:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 108/148 [04:50<01:43,  2.58s/it]\u001b[A\n",
            "Processing:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 109/148 [04:52<01:40,  2.58s/it]\u001b[A\n",
            "Processing:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 110/148 [04:55<01:39,  2.61s/it]\u001b[A\n",
            "Processing:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 111/148 [04:58<01:37,  2.65s/it]\u001b[A\n",
            "Processing:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 112/148 [05:00<01:34,  2.63s/it]\u001b[A\n",
            "Processing:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 113/148 [05:03<01:31,  2.61s/it]\u001b[A\n",
            "Processing:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 114/148 [05:05<01:28,  2.60s/it]\u001b[A\n",
            "Processing:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 115/148 [05:08<01:26,  2.62s/it]\u001b[A\n",
            "Processing:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 116/148 [05:11<01:24,  2.65s/it]\u001b[A\n",
            "Processing:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 117/148 [05:13<01:21,  2.62s/it]\u001b[A\n",
            "Processing:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 118/148 [05:16<01:18,  2.60s/it]\u001b[A\n",
            "Processing:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 119/148 [05:18<01:15,  2.59s/it]\u001b[A\n",
            "Processing:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 120/148 [05:21<01:13,  2.62s/it]\u001b[A\n",
            "Processing:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 121/148 [05:24<01:11,  2.64s/it]\u001b[A\n",
            "Processing:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 122/148 [05:26<01:08,  2.62s/it]\u001b[A\n",
            "Processing:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 123/148 [05:29<01:04,  2.59s/it]\u001b[A"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# SAM 3 Video Segmentation - Standalone (COLMAPÏö©)\n",
        "# ============================================\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# SAM 3 imports\n",
        "from sam3.model_builder import build_sam3_image_model\n",
        "from sam3.model.sam3_image_processor import Sam3Processor\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION - Ïó¨Í∏∞Îßå ÏàòÏ†ïÌïòÏÑ∏Ïöî!\n",
        "# ============================================\n",
        "\n",
        "VIDEO_PATH = \"/content/drive/MyDrive/25_sch/CV/Ass3/raw_data/IMG_7593.MOV\"  # ÎπÑÎîîÏò§ Í≤ΩÎ°ú\n",
        "OUTPUT_DIR = \"/content/ydino_0.4\"  # Ï†ÄÏû• Í≤ΩÎ°ú\n",
        "TEXT_PROMPTS = [\"yellow toy\"]  # ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖòÌï† Í∞ùÏ≤¥\n",
        "\n",
        "# Optional settings\n",
        "SKIP_SECONDS = 4  # Ï≤òÏùå NÏ¥à Í±¥ÎÑàÎõ∞Í∏∞ (ÌïôÎ≤à Ï†úÏô∏)\n",
        "SAMPLE_RATE = 6   # 1 ÌîÑÎ†àÏûÑ / N ÌîÑÎ†àÏûÑ (6 = Îß§ 6Î≤àÏß∏ ÌîÑÎ†àÏûÑ)\n",
        "CONFIDENCE_THRESHOLD = 0.4  # 0.1-0.3 Ï∂îÏ≤ú\n",
        "\n",
        "# ============================================\n",
        "# 1. Load Model\n",
        "# ============================================\n",
        "\n",
        "print(\"ü§ñ Loading SAM 3 model...\\n\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = build_sam3_image_model().to(device)\n",
        "processor = Sam3Processor(model, confidence_threshold=CONFIDENCE_THRESHOLD)\n",
        "\n",
        "print(f\"‚úÖ Model loaded on {device}\")\n",
        "print(f\"   GPU Memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\\n\")\n",
        "\n",
        "# ============================================\n",
        "# 2. Process Video\n",
        "# ============================================\n",
        "\n",
        "print(f\"üé¨ Processing video: {Path(VIDEO_PATH).name}\\n\")\n",
        "print(f\"   Output: {OUTPUT_DIR}\")\n",
        "print(f\"   Prompts: {TEXT_PROMPTS}\")\n",
        "print(f\"   Skip: {SKIP_SECONDS}s, Sample: 1/{SAMPLE_RATE}\\n\")\n",
        "\n",
        "# Create output directory\n",
        "output_path = Path(OUTPUT_DIR)\n",
        "output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Open video\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    raise ValueError(f\"Cannot open video: {VIDEO_PATH}\")\n",
        "\n",
        "# Video info\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "skip_frames = int(SKIP_SECONDS * fps)\n",
        "\n",
        "print(f\"üìä Video info:\")\n",
        "print(f\"   FPS: {fps:.1f}\")\n",
        "print(f\"   Total frames: {total_frames}\")\n",
        "print(f\"   Skip frames: {skip_frames}\")\n",
        "\n",
        "expected = (total_frames - skip_frames) // SAMPLE_RATE\n",
        "print(f\"   Expected output: ~{expected} frames\\n\")\n",
        "\n",
        "# Process frames\n",
        "frame_idx = 0\n",
        "output_idx = 0\n",
        "processed = 0\n",
        "failed = 0\n",
        "\n",
        "pbar = tqdm(total=expected, desc=\"Processing\")\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Skip initial frames (student ID)\n",
        "    if frame_idx < skip_frames:\n",
        "        frame_idx += 1\n",
        "        continue\n",
        "\n",
        "    # Sample frames\n",
        "    if (frame_idx - skip_frames) % SAMPLE_RATE != 0:\n",
        "        frame_idx += 1\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Convert BGR to RGB\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        height, width = frame_rgb.shape[:2]\n",
        "\n",
        "        # Convert to PIL\n",
        "        image_pil = Image.fromarray(frame_rgb)\n",
        "\n",
        "        # Set image (extract features)\n",
        "        inference_state = processor.set_image(image_pil)\n",
        "\n",
        "        # Initialize combined mask\n",
        "        combined_mask = np.zeros((height, width), dtype=bool)\n",
        "\n",
        "        # Try each text prompt\n",
        "        for prompt in TEXT_PROMPTS:\n",
        "            try:\n",
        "                # Reset previous prompts\n",
        "                processor.reset_all_prompts(inference_state)\n",
        "\n",
        "                # Set text prompt (PCS)\n",
        "                output = processor.set_text_prompt(\n",
        "                    state=inference_state,\n",
        "                    prompt=prompt\n",
        "                )\n",
        "\n",
        "                # Extract masks\n",
        "                if \"masks\" in output:\n",
        "                    masks = output[\"masks\"]\n",
        "\n",
        "                    # Convert tensor to list\n",
        "                    if torch.is_tensor(masks):\n",
        "                        masks = [masks[i] for i in range(len(masks))]\n",
        "\n",
        "                    # Process each mask\n",
        "                    for mask in masks:\n",
        "                        # Tensor to numpy\n",
        "                        if torch.is_tensor(mask):\n",
        "                            mask = mask.cpu().numpy()\n",
        "\n",
        "                        # Convert to boolean\n",
        "                        if mask.dtype != bool:\n",
        "                            mask = mask > 0.5\n",
        "\n",
        "                        # 3D ‚Üí 2D\n",
        "                        while mask.ndim > 2:\n",
        "                            mask = mask[0]\n",
        "\n",
        "                        # Resize if needed\n",
        "                        if mask.shape != (height, width):\n",
        "                            mask = cv2.resize(\n",
        "                                mask.astype(np.uint8),\n",
        "                                (width, height),\n",
        "                                interpolation=cv2.INTER_NEAREST\n",
        "                            ).astype(bool)\n",
        "\n",
        "                        # Combine masks (OR operation)\n",
        "                        combined_mask |= mask\n",
        "\n",
        "            except Exception as e:\n",
        "                # Skip failed prompt\n",
        "                pass\n",
        "\n",
        "        # Fallback: if no mask found, keep full image\n",
        "        if not combined_mask.any():\n",
        "            combined_mask = np.ones((height, width), dtype=bool)\n",
        "\n",
        "        # Create RGB with BLACK background (for COLMAP)\n",
        "        rgb_with_black_bg = np.zeros((height, width, 3), dtype=np.uint8)\n",
        "\n",
        "        # Copy only masked region from original frame\n",
        "        rgb_with_black_bg[combined_mask] = frame_rgb[combined_mask]\n",
        "\n",
        "        # Background is black (0,0,0) by default\n",
        "\n",
        "        # Save as PNG (3-channel RGB, not RGBA)\n",
        "        out_file = output_path / f\"frame_{output_idx:04d}.png\"\n",
        "        cv2.imwrite(str(out_file), cv2.cvtColor(rgb_with_black_bg, cv2.COLOR_RGB2BGR))\n",
        "        # ============================================\n",
        "\n",
        "        processed += 1\n",
        "        output_idx += 1\n",
        "        pbar.update(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        failed += 1\n",
        "\n",
        "    frame_idx += 1\n",
        "\n",
        "cap.release()\n",
        "pbar.close()\n",
        "\n",
        "# ============================================\n",
        "# 3. Summary\n",
        "# ============================================\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"‚úÖ PROCESSING COMPLETE\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\nüìä Results:\")\n",
        "print(f\"   Processed: {processed} frames\")\n",
        "print(f\"   Failed: {failed} frames\")\n",
        "print(f\"   Coverage: {processed / expected * 100:.1f}%\")\n",
        "print(f\"\\nüìÅ Output directory: {output_path}\")\n",
        "print(f\"\\nüéØ Files created:\")\n",
        "\n",
        "# List first 5 files\n",
        "files = sorted(list(output_path.glob(\"frame_*.png\")))\n",
        "for f in files[:5]:\n",
        "    print(f\"   {f.name}\")\n",
        "if len(files) > 5:\n",
        "    print(f\"   ... and {len(files) - 5} more\")\n",
        "\n",
        "print(f\"\\n‚úÖ Total: {len(files)} PNG files with BLACK background (COLMAP ready)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHQSTb9vzUbd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pZhNoSgsX3z"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 4. Create ZIP File\n",
        "# ============================================\n",
        "\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"üì¶ Creating ZIP file...\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# ZIP file path\n",
        "zip_name = f\"{Path(VIDEO_PATH).stem}_dino_0.2\"\n",
        "zip_file_name = f\"{zip_name}.zip\"\n",
        "zip_path = output_path.parent / zip_file_name\n",
        "\n",
        "print(f\"   Output: {zip_path.name}\")\n",
        "\n",
        "# Get all PNG files\n",
        "png_files = sorted(list(output_path.glob(\"frame_*.png\")))\n",
        "\n",
        "if not png_files:\n",
        "    print(\"   ‚ùå No PNG files found!\")\n",
        "else:\n",
        "    # Create ZIP\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for png_file in tqdm(png_files, desc=\"Zipping\"):\n",
        "            # Add to ZIP with relative path\n",
        "            zipf.write(png_file, f\"{zip_name}/{png_file.name}\")\n",
        "\n",
        "    # Print results\n",
        "    zip_size_mb = zip_path.stat().st_size / (1024 * 1024)\n",
        "    print(f\"\\n‚úÖ ZIP created successfully!\")\n",
        "    print(f\"   Files: {len(png_files)} frames\")\n",
        "    print(f\"   Size: {zip_size_mb:.1f} MB\")\n",
        "    print(f\"   Path: {zip_path}\")\n",
        "\n",
        "# ============================================\n",
        "# 5. Copy to Drive (Optional)\n",
        "# ============================================\n",
        "\n",
        "SAVE_TO_DRIVE = True  # TrueÎ°ú ÏÑ§Ï†ïÌïòÎ©¥ DriveÏóê Î≥µÏÇ¨\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/25_sch/CV/Ass3/segmented_outputs\"\n",
        "\n",
        "if SAVE_TO_DRIVE:\n",
        "    print(f\"\\nüì§ Copying to Google Drive...\")\n",
        "\n",
        "    drive_dest = Path(DRIVE_PATH)\n",
        "    drive_dest.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    drive_file = drive_dest / zip_path.name\n",
        "    shutil.copy(zip_path, drive_file)\n",
        "\n",
        "    print(f\"   ‚úÖ Saved to: {drive_file}\")\n",
        "else:\n",
        "    print(f\"\\n‚è≠Ô∏è  Skipping Drive upload (SAVE_TO_DRIVE=False)\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"üéâ ALL DONE!\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMB82szqiX5N"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# SAM 3 Batch Video Segmentation - COLMAPÏö©\n",
        "# ============================================\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# SAM 3 imports\n",
        "from sam3.model_builder import build_sam3_image_model\n",
        "from sam3.model.sam3_image_processor import Sam3Processor\n",
        "\n",
        "# ============================================\n",
        "# BATCH CONFIGURATION - Ïó¨Í∏∞Îßå ÏàòÏ†ïÌïòÏÑ∏Ïöî!\n",
        "# ============================================\n",
        "\n",
        "# ÎπÑÎîîÏò§ ÏÑ§Ï†ï Î¶¨Ïä§Ìä∏\n",
        "VIDEO_CONFIGS = [\n",
        "    {\n",
        "        'video_path': \"/content/drive/MyDrive/25_sch/CV/Ass3/raw_data/IMG_7502.MOV\",\n",
        "        'output_name': \"obj1_chimbread\",\n",
        "        'text_prompts': [\"plush with man face\"],\n",
        "        'confidence': 0.3,\n",
        "        'skip_seconds': 4,\n",
        "        'sample_rate': 6,\n",
        "    },\n",
        "    {\n",
        "        'video_path': \"/content/drive/MyDrive/25_sch/CV/Ass3/raw_data/IMG_7495.MOV\",\n",
        "        'output_name': \"obj2_wood_toy\",\n",
        "        'text_prompts': [\"wood toy\"],\n",
        "        'confidence': 0.3,\n",
        "        'skip_seconds': 4,\n",
        "        'sample_rate': 6,\n",
        "    },\n",
        "    {\n",
        "        'video_path': \"/content/drive/MyDrive/25_sch/CV/Ass3/raw_data/IMG_7492.MOV\",\n",
        "        'output_name': \"obj3_dragon\",\n",
        "        'text_prompts': [\"yellow dino\"],\n",
        "        'confidence': 0.3,\n",
        "        'skip_seconds': 4,\n",
        "        'sample_rate': 6,\n",
        "    },\n",
        "]\n",
        "\n",
        "# Ï†ÑÏó≠ ÏÑ§Ï†ï\n",
        "BASE_OUTPUT_DIR = \"/content/segmented_outputs\"\n",
        "DRIVE_SAVE_PATH = \"/content/drive/MyDrive/25_sch/CV/Ass3/segmented_outputs\"\n",
        "SAVE_TO_DRIVE = True\n",
        "\n",
        "# ============================================\n",
        "# 1. Load Model (Ìïú Î≤àÎßå)\n",
        "# ============================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ü§ñ Loading SAM 3 model...\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = build_sam3_image_model().to(device)\n",
        "\n",
        "print(f\"‚úÖ Model loaded on {device}\")\n",
        "print(f\"   GPU Memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\\n\")\n",
        "\n",
        "# ============================================\n",
        "# 2. Process Each Video\n",
        "# ============================================\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for idx, config in enumerate(VIDEO_CONFIGS, 1):\n",
        "    print(\"=\"*60)\n",
        "    print(f\"üìπ Video {idx}/{len(VIDEO_CONFIGS)}: {config['output_name']}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    video_path = config['video_path']\n",
        "    output_name = config['output_name']\n",
        "    text_prompts = config['text_prompts']\n",
        "    confidence = config['confidence']\n",
        "    skip_seconds = config['skip_seconds']\n",
        "    sample_rate = config['sample_rate']\n",
        "\n",
        "    # Create processor with specific confidence\n",
        "    processor = Sam3Processor(model, confidence_threshold=confidence)\n",
        "\n",
        "    print(f\"\\nüìã Configuration:\")\n",
        "    print(f\"   Video: {Path(video_path).name}\")\n",
        "    print(f\"   Prompts: {text_prompts}\")\n",
        "    print(f\"   Confidence: {confidence}\")\n",
        "    print(f\"   Skip: {skip_seconds}s, Sample: 1/{sample_rate}\")\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir = Path(BASE_OUTPUT_DIR) / output_name\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Open video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(f\"‚ùå Cannot open video: {video_path}\")\n",
        "        all_results.append({\n",
        "            'name': output_name,\n",
        "            'status': 'failed',\n",
        "            'error': 'Cannot open video'\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    # Video info\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    skip_frames = int(skip_seconds * fps)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    print(f\"\\nüìä Video info:\")\n",
        "    print(f\"   Resolution: {width}x{height}\")\n",
        "    print(f\"   FPS: {fps:.1f}\")\n",
        "    print(f\"   Total frames: {total_frames}\")\n",
        "    print(f\"   Skip frames: {skip_frames}\")\n",
        "\n",
        "    expected = (total_frames - skip_frames) // sample_rate\n",
        "    print(f\"   Expected output: ~{expected} frames\\n\")\n",
        "\n",
        "    # Process frames\n",
        "    frame_idx = 0\n",
        "    output_idx = 0\n",
        "    processed = 0\n",
        "    failed = 0\n",
        "\n",
        "    pbar = tqdm(total=expected, desc=f\"Processing {output_name}\")\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Skip initial frames\n",
        "        if frame_idx < skip_frames:\n",
        "            frame_idx += 1\n",
        "            continue\n",
        "\n",
        "        # Sample frames\n",
        "        if (frame_idx - skip_frames) % sample_rate != 0:\n",
        "            frame_idx += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Convert BGR to RGB\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            image_pil = Image.fromarray(frame_rgb)\n",
        "\n",
        "            # SAM 3 segmentation\n",
        "            inference_state = processor.set_image(image_pil)\n",
        "            combined_mask = np.zeros((height, width), dtype=bool)\n",
        "\n",
        "            # Try each text prompt\n",
        "            for prompt in text_prompts:\n",
        "                try:\n",
        "                    processor.reset_all_prompts(inference_state)\n",
        "                    output = processor.set_text_prompt(\n",
        "                        state=inference_state,\n",
        "                        prompt=prompt\n",
        "                    )\n",
        "\n",
        "                    if \"masks\" in output:\n",
        "                        masks = output[\"masks\"]\n",
        "\n",
        "                        if torch.is_tensor(masks):\n",
        "                            masks = [masks[i] for i in range(len(masks))]\n",
        "\n",
        "                        for mask in masks:\n",
        "                            if torch.is_tensor(mask):\n",
        "                                mask = mask.cpu().numpy()\n",
        "\n",
        "                            if mask.dtype != bool:\n",
        "                                mask = mask > 0.5\n",
        "\n",
        "                            while mask.ndim > 2:\n",
        "                                mask = mask[0]\n",
        "\n",
        "                            if mask.shape != (height, width):\n",
        "                                mask = cv2.resize(\n",
        "                                    mask.astype(np.uint8),\n",
        "                                    (width, height),\n",
        "                                    interpolation=cv2.INTER_NEAREST\n",
        "                                ).astype(bool)\n",
        "\n",
        "                            combined_mask |= mask\n",
        "\n",
        "                except Exception as e:\n",
        "                    pass\n",
        "\n",
        "            # Fallback: keep full image if no mask\n",
        "            if not combined_mask.any():\n",
        "                combined_mask = np.ones((height, width), dtype=bool)\n",
        "\n",
        "            # Create RGB with black background\n",
        "            rgb_with_black_bg = np.zeros((height, width, 3), dtype=np.uint8)\n",
        "            rgb_with_black_bg[combined_mask] = frame_rgb[combined_mask]\n",
        "\n",
        "            # Save as PNG\n",
        "            out_file = output_dir / f\"frame_{output_idx:04d}.png\"\n",
        "            cv2.imwrite(str(out_file), cv2.cvtColor(rgb_with_black_bg, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "            processed += 1\n",
        "            output_idx += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            failed += 1\n",
        "\n",
        "        frame_idx += 1\n",
        "\n",
        "    cap.release()\n",
        "    pbar.close()\n",
        "\n",
        "    # ============================================\n",
        "    # 3. Create ZIP for this video\n",
        "    # ============================================\n",
        "\n",
        "    print(f\"\\nüì¶ Creating ZIP file...\")\n",
        "\n",
        "    zip_name = f\"{output_name}_segmented_c{confidence}.zip\"\n",
        "    zip_path = Path(BASE_OUTPUT_DIR) / zip_name\n",
        "\n",
        "    png_files = sorted(list(output_dir.glob(\"frame_*.png\")))\n",
        "\n",
        "    if png_files:\n",
        "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            for png_file in tqdm(png_files, desc=\"Zipping\", leave=False):\n",
        "                zipf.write(png_file, f\"{output_name}/images/{png_file.name}\")\n",
        "\n",
        "        zip_size_mb = zip_path.stat().st_size / (1024 * 1024)\n",
        "\n",
        "        print(f\"   ‚úÖ ZIP created: {zip_name}\")\n",
        "        print(f\"   Files: {len(png_files)} frames\")\n",
        "        print(f\"   Size: {zip_size_mb:.1f} MB\")\n",
        "\n",
        "        # Save to Drive\n",
        "        if SAVE_TO_DRIVE:\n",
        "            drive_dest = Path(DRIVE_SAVE_PATH)\n",
        "            drive_dest.mkdir(parents=True, exist_ok=True)\n",
        "            drive_file = drive_dest / zip_name\n",
        "            shutil.copy(zip_path, drive_file)\n",
        "            print(f\"   üì§ Saved to Drive: {drive_file.name}\")\n",
        "\n",
        "        # Store results\n",
        "        all_results.append({\n",
        "            'name': output_name,\n",
        "            'status': 'success',\n",
        "            'video': Path(video_path).name,\n",
        "            'frames': processed,\n",
        "            'failed': failed,\n",
        "            'prompts': text_prompts,\n",
        "            'confidence': confidence,\n",
        "            'zip_path': str(zip_path),\n",
        "            'zip_size_mb': zip_size_mb\n",
        "        })\n",
        "    else:\n",
        "        print(f\"   ‚ùå No frames extracted!\")\n",
        "        all_results.append({\n",
        "            'name': output_name,\n",
        "            'status': 'failed',\n",
        "            'error': 'No frames extracted'\n",
        "        })\n",
        "\n",
        "    print()\n",
        "\n",
        "# ============================================\n",
        "# 4. Final Summary\n",
        "# ============================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üéâ BATCH PROCESSING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìä Summary:\")\n",
        "print(f\"   Total videos: {len(VIDEO_CONFIGS)}\")\n",
        "print(f\"   Successful: {sum(1 for r in all_results if r['status'] == 'success')}\")\n",
        "print(f\"   Failed: {sum(1 for r in all_results if r['status'] == 'failed')}\")\n",
        "\n",
        "print(f\"\\nüìã Results:\\n\")\n",
        "\n",
        "for result in all_results:\n",
        "    if result['status'] == 'success':\n",
        "        print(f\"‚úÖ {result['name']}\")\n",
        "        print(f\"   Video: {result['video']}\")\n",
        "        print(f\"   Frames: {result['frames']} (failed: {result['failed']})\")\n",
        "        print(f\"   Prompts: {result['prompts']}\")\n",
        "        print(f\"   Confidence: {result['confidence']}\")\n",
        "        print(f\"   ZIP: {Path(result['zip_path']).name} ({result['zip_size_mb']:.1f} MB)\")\n",
        "    else:\n",
        "        print(f\"‚ùå {result['name']}\")\n",
        "        print(f\"   Error: {result.get('error', 'Unknown')}\")\n",
        "    print()\n",
        "\n",
        "if SAVE_TO_DRIVE:\n",
        "    print(f\"üìÅ All files saved to: {DRIVE_SAVE_PATH}\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üéØ Next Steps:\")\n",
        "print(\"   1. Download ZIP files from Drive\")\n",
        "print(\"   2. Extract to colmap_input/\")\n",
        "print(\"   3. Run COLMAP on each dataset\")\n",
        "print(\"   4. Train 3DGS models\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
